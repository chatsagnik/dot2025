@article{ChatterjeeBhatia23QuantumBoosting,
  title = {Quantum Boosting Using Domain-Partitioning Hypotheses},
  author = {Chatterjee, Sagnik and Bhatia, Rohan and Singh Chani, Parmeet and Bera, Debajyoti},
  year = {2023},
  month = jul,
  journal = {Quantum Machine Intelligence},
  volume = {5},
  number = {2},
  pages = {33},
  issn = {2524-4914},
  doi = {10.1007/s42484-023-00122-3},
  urldate = {2025-01-09},
  abstract = {Boosting is an ensemble learning method that converts a weak learner into a strong learner in the PAC learning framework. The AdaBoost algorithm is a well-known classical boosting algorithm for weak learners with binary hypotheses. Recently, Arunachalam and Maity presented the first quantum boosting algorithm by quantizing AdaBoost. Their algorithm, which we refer to as QAdaBoost hereafter, is a quantum adaptation of AdaBoost and only works for the binary hypothesis case. QAdaBoost is quadratically faster than AdaBoost in terms of the VC dimension of the hypothesis class of the weak learner. However, QAdaBoost is polynomially worse in the bias of the weak learner. In this work, we address an open question by Izdebski et al. on the existence of boosting algorithms for quantum weak learners with non-binary hypotheses. We answer this question in the affirmative by developing the QRealBoost algorithm motivated by the classical RealBoost algorithm. The main technical challenge was to provide provable guarantees for convergence, generalization bounds, and quantum speedup, given that quantum subroutines are noisy and probabilistic. We prove that QRealBoost retains the quadratic speedup of QAdaBoost over AdaBoost and further achieves a polynomial speedup over QAdaBoost in terms of both the bias of the learner and the time taken by the learner to learn the target concept class. Finally, we perform empirical evaluations on QRealBoost and report encouraging observations on quantum simulators by benchmarking the convergence performance of QRealBoost against QAdaBoost, AdaBoost, RealBoost, and SmoothBoost on a subset of the MNIST dataset and Breast Cancer Wisconsin dataset.},
  langid = {english},
  keywords = {Artificial Intelligence,Boosting,Quantum computing,Quantum Computing},
  file = {/home/chatsagnik/Zotero/storage/SEM2344T/Chatterjee et al. - 2023 - Quantum boosting using domain-partitioning hypotheses.pdf}
}

@misc{ChatterjeeMukherjee24GeneralizationBounds,
  title = {Generalization {{Bounds}} for {{Dependent Data}} Using {{Online-to-Batch Conversion}}},
  author = {Chatterjee, Sagnik and Mukherjee, Manuj and Sethi, Alhad},
  year = {2024},
  month = may,
  number = {arXiv:2405.13666},
  eprint = {2405.13666},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.13666},
  urldate = {2025-01-09},
  abstract = {In this work, we give generalization bounds of statistical learning algorithms trained on samples drawn from a dependent data source both in expectation and with high probability, using the Online-to-Batch conversion paradigm. We show that the generalization error of statistical learners in the dependent data setting is equivalent to the generalization error of statistical learners in the i.i.d. setting up to a term that depends on the decay rate of the underlying mixing stochastic process, and is independent of the complexity of the statistical learner. Our proof techniques involve defining a new notion of stability of online learning algorithms based on Wasserstein distances, and employing ''near-martingale'' concentration bounds for dependent random variables to arrive at appropriate upper bounds for the generalization error of statistical learners trained on dependent data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/chatsagnik/Zotero/storage/8QGG5M9H/Chatterjee et al. - 2024 - Generalization Bounds for Dependent Data using Online-to-Batch Conversion.pdf}
}

@inproceedings{ChatterjeeSapv24EfficientQuantum,
  title = {Efficient {{Quantum Agnostic Improper Learning}} of {{Decision Trees}}},
  booktitle = {Proceedings of {{The}} 27th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Chatterjee, Sagnik and Sapv, Tharrmashastha and Bera, Debajyoti},
  year = {2024},
  month = apr,
  pages = {514--522},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-01-09},
  abstract = {The agnostic setting is the hardest generalization of the PAC model since it is akin to learning with adversarial noise. In this paper, we give a poly (n,t,1/{$\epsilon$})(n,t,1/{$\epsilon$})(n, t, 1/{\textbackslash}epsilon) quantum algorithm for learning size ttt decision trees over nnn-bit inputs with uniform marginal over instances, in the agnostic setting, without membership queries (MQ). This is the first algorithm (classical or quantum) for efficiently learning decision trees without MQ. First, we construct a quantum agnostic weak learner by designing a quantum variant of the classical Goldreich-Levin algorithm that works with strongly biased function oracles. Next, we show how to quantize the agnostic boosting algorithm by Kalai and Kanade (2009) to obtain the first efficient quantum agnostic boosting algorithm (that has a polynomial speedup over existing adaptive quantum boosting algorithms). We then use the quantum agnostic boosting algorithm to boost the weak quantum agnostic learner constructed previously to obtain a quantum agnostic learner for decision trees. Using the above framework, we also give quantum decision tree learning algorithms without MQ in weaker noise models.},
  langid = {english},
  file = {/home/chatsagnik/Zotero/storage/M5Q8G7E7/Chatterjee et al. - 2024 - Efficient Quantum Agnostic Improper Learning of Decision Trees.pdf}
}
